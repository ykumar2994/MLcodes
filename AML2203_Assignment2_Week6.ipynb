{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30b6ea4",
   "metadata": {},
   "source": [
    "# AML 2203 Assignment 2\n",
    "# Student ID - C0852435\n",
    "# Student name - Yogesh Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134f78a",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0c653",
   "metadata": {},
   "source": [
    "# Cleaning HTML Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d80e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is an example', 'is an example to', 'an example to extract', 'example to extract n-grams']\n",
      "['this is an example to', 'is an example to extract', 'an example to extract n-grams']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"this is an example to extract n-grams\"\n",
    "\n",
    "#function to generate n-grams\n",
    "def extarct_ngrams(data,num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "#4-grams\n",
    "print(extarct_ngrams(text,4))\n",
    "#5-grams\n",
    "print(extarct_ngrams(text, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d05f8e",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b8694",
   "metadata": {},
   "source": [
    "### Using word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620b4368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'Welcome', 'to', 'Word', 'Tokenization']\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize as nt\n",
    "\n",
    "text1 = \"Hello everyone. Welcome to Word Tokenization\"\n",
    "\n",
    "words = nt.word_tokenize(text1)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0201ea6",
   "metadata": {},
   "source": [
    "### Using tokenize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2516d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone.', 'Welcome', 'to', 'Word', 'Tokenization']\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize as nt\n",
    "\n",
    "text1 = \"Hello everyone. Welcome to Word Tokenization\"\n",
    "\n",
    "tokenizer = nt.TreebankWordTokenizer()\n",
    "words1 = tokenizer.tokenize(text1)\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bbaa7",
   "metadata": {},
   "source": [
    "# Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ac9bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'are', 'with', 'needn', \"needn't\", \"it's\", \"haven't\", 'out', \"you've\", 'why', 'there', 'have', 'am', 'too', 'down', 'as', 'by', \"doesn't\", \"won't\", 'in', 'against', 'we', 'does', 'between', 'y', 'no', 'where', 'now', 'what', 'and', 'again', 'on', 'all', 'himself', 'myself', 'hers', 'its', 'i', \"shan't\", \"you'd\", 'those', 'to', 'both', 'doing', \"don't\", 'isn', 'themselves', 'do', \"mightn't\", 'll', 'but', 'shan', 'don', 'here', 'each', 'ain', 'is', 'or', 'the', 'if', 'about', 'during', 'their', \"shouldn't\", \"didn't\", 'into', \"mustn't\", 'who', 'while', 'nor', 'an', 'ma', 'above', 'from', 'after', 'be', 'our', 're', 'weren', \"couldn't\", 'has', 'haven', 'won', \"weren't\", 'mustn', 'few', 'so', \"you're\", 'wasn', 'her', 'then', 'were', 'that', 'did', 'him', 'ourselves', 'up', 'when', 'than', 'through', 'over', 'just', 'should', 'will', 'own', 'because', 'only', 'herself', 'below', 'off', 'how', 'yours', 'been', \"wasn't\", 'these', 'you', 'this', \"you'll\", 'itself', \"wouldn't\", 'm', 'me', 'same', 'she', 'they', 'most', 'aren', 'your', 's', \"she's\", 'whom', 'it', 'until', 't', 'under', 'further', \"isn't\", 'having', 'my', 'couldn', 'ours', 'which', 'more', 'not', 'hasn', 'theirs', 'of', 'such', 'was', 'very', 'them', 've', 'being', 'once', 'can', 'any', 'before', 'doesn', 'had', 'yourselves', 'he', 'yourself', 'shouldn', \"hadn't\", 'at', 'd', 'mightn', 'wouldn', 'a', 'didn', 'some', \"that'll\", 'hadn', \"hasn't\", \"should've\", 'o', 'other', 'his', \"aren't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ce9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'was', 'no', 'chance', 'for', 'any', 'character', 'development', 'they', 'were', 'busy', '.']\n",
      "['There', 'chance', 'character', 'development', 'busy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"There was no chance for any character development they were busy.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(text)\n",
    "result = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "        \n",
    "print(words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b99a4",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534e9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: \n",
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "destabilize  :  destabil\n",
      "\n",
      "Lancaster Stemmer : \n",
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  program\n",
      "programming  :  program\n",
      "destabilize  :  dest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "words = [\"program\",\"programs\",\"programmer\",\"programming\",\"destabilize\"]\n",
    "\n",
    "print('Porter Stemmer: ')\n",
    "for w in words:\n",
    "    print(w,\" : \",ps.stem(w))\n",
    "\n",
    "print('\\nLancaster Stemmer : ')\n",
    "for w in words:\n",
    "    print(w,\" : \",ls.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa99a65",
   "metadata": {},
   "source": [
    "# Bag of Words - NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433a3a7",
   "metadata": {},
   "source": [
    "### 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb728e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie made it into one of my top 10 most awful movies \n",
      "there wasn t a continous minute where there wasn t a fight with one monster or another \n",
      "there was no chance for any character development they were too busy running from one sword fight to another \n",
      "i had no emotional attachment except to the big machine that wanted to destroy them \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "text = \"\"\"This movie made it into one of my top 10 most awful movies.\n",
    "There wasn't a continous minute where there wasn't a fight with one monster or another.\n",
    "There was no chance for any character development, they were too busy running from one sword fight to another.\n",
    "I had no emotional attachment ( except to the big machine ## that wanted to destroy them).\"\"\"\n",
    "\n",
    "data = nltk.sent_tokenize(text)\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].lower()\n",
    "    data[i] = re.sub(r'\\W+',' ', data[i])\n",
    "    data[i] = re.sub(r'\\s+',' ', data[i])\n",
    "    \n",
    "for i in data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87a50d",
   "metadata": {},
   "source": [
    "### 2. Creation of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab642da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "['one', 'there', 'to', 'wasn', 't', 'a', 'fight', 'another', 'no', 'this', 'movie', 'made', 'it', 'into', 'of', 'my', 'top', '10', 'most', 'awful', 'movies', 'continous', 'minute', 'where', 'with', 'monster', 'or', 'was', 'chance', 'for']\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "for i in data:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in wordcount.keys():\n",
    "            wordcount[word] = 1\n",
    "        else:\n",
    "            wordcount[word] += 1\n",
    "            \n",
    "print(len(wordcount))\n",
    "\n",
    "freq_words = heapq.nlargest(30, wordcount, key=wordcount.get)\n",
    "print(freq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d30d1",
   "metadata": {},
   "source": [
    "### 3. Creation of Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7879dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0]\n",
      " [1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "x= []\n",
    "for i in data:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(i):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    x.append(vector)\n",
    "x = np.asarray(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72792cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
